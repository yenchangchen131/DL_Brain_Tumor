{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Brain Tumor Segmentation - GPU Safe Mode\n",
                "\n",
                "## âš ï¸ GTX 960 (4GB) å„ªåŒ–é…ç½®\n",
                "\n",
                "æœ¬ç‰ˆæœ¬é‡å°æ‚¨çš„é¡¯å¡é€²è¡Œäº†ä»¥ä¸‹å„ªåŒ–ï¼š\n",
                "- âœ… æ¥µå°çš„ batch size (2)\n",
                "- âœ… é™ä½çš„åœ–ç‰‡è§£æåº¦ (256x256)\n",
                "- âœ… GPU è¨˜æ†¶é«”è‡ªå‹•æ¸…ç†\n",
                "- âœ… CUDA TDR éŒ¯èª¤è™•ç†\n",
                "- âœ… è‡ªå‹•é™ç´šåˆ° CPUï¼ˆå¦‚æœ GPU å¤±æ•—ï¼‰\n",
                "\n",
                "## é‡è¦æé†’\n",
                "1. **é‹è¡Œå‰è«‹é—œé–‰å…¶ä»–ä½”ç”¨ GPU çš„ç¨‹å¼**\n",
                "2. **å¦‚æœé‡åˆ°éŒ¯èª¤ï¼Œæœƒè‡ªå‹•åˆ‡æ›åˆ° CPU**\n",
                "3. **è¨“ç·´æœƒæ¯”è¼ƒæ…¢ï¼Œä½†æ‡‰è©²ä¸æœƒå´©æ½°**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "CUDA å¯ç”¨: True\n",
                        "GPU åç¨±: NVIDIA GeForce GTX 960\n",
                        "GPU è¨˜æ†¶é«”: 4.00 GB\n",
                        "âœ… GPU å·²åˆå§‹åŒ–ä¸¦æ¸…ç†è¨˜æ†¶é«”\n",
                        "âš ï¸  å¦‚æœè¨“ç·´æ™‚é‡åˆ°éŒ¯èª¤ï¼Œå°‡è‡ªå‹•åˆ‡æ›åˆ° CPU\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "# ========================================\n",
                "# GPU å®‰å…¨åˆå§‹åŒ– - é‡å° GTX 960\n",
                "# ========================================\n",
                "import torch\n",
                "import gc\n",
                "import os\n",
                "\n",
                "# æ¸…ç†è¨˜æ†¶é«”\n",
                "gc.collect()\n",
                "\n",
                "# æª¢æŸ¥ CUDA å¯ç”¨æ€§\n",
                "cuda_available = torch.cuda.is_available()\n",
                "print(\"=\"*60)\n",
                "print(f\"CUDA å¯ç”¨: {cuda_available}\")\n",
                "\n",
                "if cuda_available:\n",
                "    print(f\"GPU åç¨±: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
                "    \n",
                "    # æ¸…ç©º CUDA cache\n",
                "    torch.cuda.empty_cache()\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "    \n",
                "    # è¨­ç½® CUDA é¸é …ä»¥é¿å… TDR\n",
                "    torch.backends.cudnn.benchmark = False  # é—œé–‰è‡ªå‹•å„ªåŒ–ï¼Œé¿å…é•·æ™‚é–“è¨ˆç®—\n",
                "    torch.backends.cudnn.deterministic = True  # ç¢ºä¿çµæœå¯é‡ç¾\n",
                "    \n",
                "    print(\"âœ… GPU å·²åˆå§‹åŒ–ä¸¦æ¸…ç†è¨˜æ†¶é«”\")\n",
                "    print(\"âš ï¸  å¦‚æœè¨“ç·´æ™‚é‡åˆ°éŒ¯èª¤ï¼Œå°‡è‡ªå‹•åˆ‡æ›åˆ° CPU\")\n",
                "else:\n",
                "    print(\"âŒ CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU\")\n",
                "    \n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "GTX 960 å„ªåŒ–é…ç½®:\n",
                        "  åœ–ç‰‡å¤§å°: 256x256\n",
                        "  Batch Size: 2 (éå¸¸å°ï¼Œé¿å… GPU è¶…æ™‚)\n",
                        "  Epochs: 25\n",
                        "  Learning Rate: 0.0001\n",
                        "  Workers: 0\n",
                        "âœ… CUDA seeds è¨­ç½®æˆåŠŸ\n",
                        "\n",
                        "ä½¿ç”¨è¨­å‚™: cuda\n"
                    ]
                }
            ],
            "source": [
                "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
                "import numpy as np\n",
                "import random\n",
                "from pathlib import Path\n",
                "\n",
                "# GTX 960 å®‰å…¨é…ç½®\n",
                "IMG_SIZE = 256      # é™ä½åˆ° 256 (åŸæœ¬å¯èƒ½æ˜¯ 640)\n",
                "BATCH_SIZE = 2      # æ¥µå°çš„ batch size\n",
                "EPOCHS = 25         # å®Œæ•´è¨“ç·´\n",
                "LR = 1e-4\n",
                "SEED = 42\n",
                "NUM_WORKERS = 0     # Windows å»ºè­°è¨­ç‚º 0\n",
                "\n",
                "print(\"GTX 960 å„ªåŒ–é…ç½®:\")\n",
                "print(f\"  åœ–ç‰‡å¤§å°: {IMG_SIZE}x{IMG_SIZE}\")\n",
                "print(f\"  Batch Size: {BATCH_SIZE} (éå¸¸å°ï¼Œé¿å… GPU è¶…æ™‚)\")\n",
                "print(f\"  Epochs: {EPOCHS}\")\n",
                "print(f\"  Learning Rate: {LR}\")\n",
                "print(f\"  Workers: {NUM_WORKERS}\")\n",
                "\n",
                "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "# å®‰å…¨åœ°è¨­ç½® CUDA seed\n",
                "if cuda_available:\n",
                "    try:\n",
                "        torch.cuda.manual_seed(SEED)\n",
                "        torch.cuda.manual_seed_all(SEED)\n",
                "        print(\"âœ… CUDA seeds è¨­ç½®æˆåŠŸ\")\n",
                "    except RuntimeError as e:\n",
                "        print(f\"âš ï¸  CUDA seed è¨­ç½®å¤±æ•—: {e}\")\n",
                "        print(\"   å°‡ç¹¼çºŒä½¿ç”¨ CPU...\")\n",
                "        cuda_available = False\n",
                "\n",
                "# è¨­å®š device\n",
                "DEVICE = 'cuda' if cuda_available else 'cpu'\n",
                "print(f\"\\nä½¿ç”¨è¨­å‚™: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# è³‡æ–™æ¢ç´¢ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "è³‡æ–™é›†çµ±è¨ˆ:\n",
                        "  åœ–ç‰‡æ•¸é‡: 1502\n",
                        "  æ¨™è¨»æ•¸é‡: 1502\n",
                        "  é¡åˆ¥æ•¸é‡: 3\n",
                        "  å„é¡åˆ¥æ¨™è¨»æ•¸: {1: 771, 2: 731}\n"
                    ]
                }
            ],
            "source": [
                "# ç°¡å–®æª¢æŸ¥è³‡æ–™é›†\n",
                "import json\n",
                "from collections import Counter\n",
                "\n",
                "path = r\".\\train\\_annotations.coco.json\"\n",
                "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
                "    data = json.load(f)\n",
                "\n",
                "print(\"è³‡æ–™é›†çµ±è¨ˆ:\")\n",
                "print(f\"  åœ–ç‰‡æ•¸é‡: {len(data.get('images', []))}\")\n",
                "print(f\"  æ¨™è¨»æ•¸é‡: {len(data.get('annotations', []))}\")\n",
                "print(f\"  é¡åˆ¥æ•¸é‡: {len(data.get('categories', []))}\")\n",
                "\n",
                "cat_counts = Counter(a[\"category_id\"] for a in data.get(\"annotations\", []))\n",
                "print(f\"  å„é¡åˆ¥æ¨™è¨»æ•¸: {dict(cat_counts)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âš ï¸ è·³éåœ–ç‰‡é¡¯ç¤ºéƒ¨åˆ†\n",
                "\n",
                "ç‚ºäº†é¿å…è¨˜æ†¶é«”å•é¡Œå°è‡´ kernel å´©æ½°ï¼Œæˆ‘å€‘å…ˆè·³éè³‡æ–™è¦–è¦ºåŒ–ï¼Œç›´æ¥é€²å…¥æ¨¡å‹è¨“ç·´ã€‚\n",
                "\n",
                "å¦‚æœæ‚¨éœ€è¦çœ‹åœ–ç‰‡ï¼Œè«‹åœ¨è¨“ç·´å®Œæˆå¾Œå†åŸ·è¡Œè¦–è¦ºåŒ–çš„ cellsã€‚"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# æ¨¡å‹è¨“ç·´\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… å¥—ä»¶å°å…¥æˆåŠŸ\n"
                    ]
                }
            ],
            "source": [
                "# å°å…¥è¨“ç·´æ‰€éœ€çš„å¥—ä»¶\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchvision.transforms as T\n",
                "from PIL import Image\n",
                "import cv2\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "print(\"âœ… å¥—ä»¶å°å…¥æˆåŠŸ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Dataset é¡åˆ¥å®šç¾©å®Œæˆ\n"
                    ]
                }
            ],
            "source": [
                "# Dataset å®šç¾©ï¼ˆç°¡åŒ–ç‰ˆï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œï¼‰\n",
                "class BrainTumorDataset(Dataset):\n",
                "    def __init__(self, image_dir, annotation_file, transform=None, size=256):\n",
                "        self.image_dir = Path(image_dir)\n",
                "        self.transform = transform\n",
                "        self.size = size\n",
                "        \n",
                "        # è¼‰å…¥æ¨™è¨»\n",
                "        with open(annotation_file, 'r') as f:\n",
                "            self.coco_data = json.load(f)\n",
                "        \n",
                "        self.images = self.coco_data['images']\n",
                "        self.annotations = self.coco_data['annotations']\n",
                "        \n",
                "        # å»ºç«‹ image_id åˆ° annotations çš„æ˜ å°„\n",
                "        self.img_to_anns = {}\n",
                "        for ann in self.annotations:\n",
                "            img_id = ann['image_id']\n",
                "            if img_id not in self.img_to_anns:\n",
                "                self.img_to_anns[img_id] = []\n",
                "            self.img_to_anns[img_id].append(ann)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_info = self.images[idx]\n",
                "        img_path = self.image_dir / img_info['file_name']\n",
                "        \n",
                "        # è®€å–åœ–ç‰‡\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        \n",
                "        # å‰µå»º mask\n",
                "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
                "        \n",
                "        # å¡«å…… mask\n",
                "        if img_info['id'] in self.img_to_anns:\n",
                "            for ann in self.img_to_anns[img_info['id']]:\n",
                "                if 'segmentation' in ann:\n",
                "                    for seg in ann['segmentation']:\n",
                "                        poly = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
                "                        cv2.fillPoly(mask, [poly], 1)\n",
                "        \n",
                "        mask = Image.fromarray(mask)\n",
                "        \n",
                "        # æ‡‰ç”¨è½‰æ›\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "            mask = T.Resize((self.size, self.size), interpolation=T.InterpolationMode.NEAREST)(mask)\n",
                "            mask = T.ToTensor()(mask)\n",
                "        \n",
                "        return image, mask\n",
                "\n",
                "print(\"âœ… Dataset é¡åˆ¥å®šç¾©å®Œæˆ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… è¨“ç·´é›†å¤§å°: 1502\n",
                        "âœ… DataLoader å‰µå»ºå®Œæˆ (batch_size=2)\n"
                    ]
                }
            ],
            "source": [
                "# æ•¸æ“šè½‰æ›ï¼ˆé™ä½è§£æåº¦ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰\n",
                "train_transform = T.Compose([\n",
                "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
                "    T.ToTensor(),\n",
                "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# å‰µå»º datasetï¼ˆåƒ…ä½¿ç”¨è¨“ç·´é›†çš„ä¸€éƒ¨åˆ†ä»¥åŠ å¿«é€Ÿåº¦ï¼‰\n",
                "train_dataset = BrainTumorDataset(\n",
                "    image_dir='./train',\n",
                "    annotation_file='./train/_annotations.coco.json',\n",
                "    transform=train_transform,\n",
                "    size=IMG_SIZE\n",
                ")\n",
                "\n",
                "print(f\"âœ… è¨“ç·´é›†å¤§å°: {len(train_dataset)}\")\n",
                "\n",
                "# å‰µå»º DataLoaderï¼ˆbatch size æ¥µå°ï¼‰\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=True,\n",
                "    num_workers=NUM_WORKERS,\n",
                "    pin_memory=False  # é¿å…é¡å¤–çš„è¨˜æ†¶é«”ä½¿ç”¨\n",
                ")\n",
                "\n",
                "print(f\"âœ… DataLoader å‰µå»ºå®Œæˆ (batch_size={BATCH_SIZE})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… æ¨¡å‹å·²ç§»åˆ° cuda\n",
                        "   GPU è¨˜æ†¶é«”ä½¿ç”¨: 7.47 MB\n",
                        "   æ¨¡å‹åƒæ•¸é‡: 1,949,697\n"
                    ]
                }
            ],
            "source": [
                "# ç°¡å–®çš„ UNet æ¨¡å‹ï¼ˆè¼•é‡åŒ–ç‰ˆæœ¬ï¼‰\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class SimpleUNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Encoderï¼ˆæ¸›å°‘é€šé“æ•¸ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰\n",
                "        self.enc1 = self.conv_block(3, 32)\n",
                "        self.enc2 = self.conv_block(32, 64)\n",
                "        self.enc3 = self.conv_block(64, 128)\n",
                "        \n",
                "        # Bottleneck\n",
                "        self.bottleneck = self.conv_block(128, 256)\n",
                "        \n",
                "        # Decoder\n",
                "        self.dec3 = self.conv_block(256 + 128, 128)\n",
                "        self.dec2 = self.conv_block(128 + 64, 64)\n",
                "        self.dec1 = self.conv_block(64 + 32, 32)\n",
                "        \n",
                "        # Output\n",
                "        self.out = nn.Conv2d(32, 1, kernel_size=1)\n",
                "        \n",
                "        self.pool = nn.MaxPool2d(2)\n",
                "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
                "    \n",
                "    def conv_block(self, in_ch, out_ch):\n",
                "        return nn.Sequential(\n",
                "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
                "            nn.BatchNorm2d(out_ch),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
                "            nn.BatchNorm2d(out_ch),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Encoder\n",
                "        e1 = self.enc1(x)\n",
                "        e2 = self.enc2(self.pool(e1))\n",
                "        e3 = self.enc3(self.pool(e2))\n",
                "        \n",
                "        # Bottleneck\n",
                "        b = self.bottleneck(self.pool(e3))\n",
                "        \n",
                "        # Decoder\n",
                "        d3 = self.dec3(torch.cat([self.upsample(b), e3], dim=1))\n",
                "        d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))\n",
                "        d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))\n",
                "        \n",
                "        return self.out(d1)\n",
                "\n",
                "# å‰µå»ºæ¨¡å‹ä¸¦ç§»å‹•åˆ°è¨­å‚™\n",
                "model = SimpleUNet()\n",
                "\n",
                "# å®‰å…¨åœ°å°‡æ¨¡å‹ç§»åˆ° GPU\n",
                "try:\n",
                "    model = model.to(DEVICE)\n",
                "    print(f\"âœ… æ¨¡å‹å·²ç§»åˆ° {DEVICE}\")\n",
                "    \n",
                "    if DEVICE == 'cuda':\n",
                "        # æ¸…ç†è¨˜æ†¶é«”\n",
                "        torch.cuda.empty_cache()\n",
                "        # é¡¯ç¤ºè¨˜æ†¶é«”ä½¿ç”¨\n",
                "        print(f\"   GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
                "except RuntimeError as e:\n",
                "    print(f\"âŒ ç„¡æ³•å°‡æ¨¡å‹ç§»åˆ° GPU: {e}\")\n",
                "    print(\"   åˆ‡æ›åˆ° CPU...\")\n",
                "    DEVICE = 'cpu'\n",
                "    model = model.to(DEVICE)\n",
                "\n",
                "# è¨ˆç®—åƒæ•¸é‡\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"   æ¨¡å‹åƒæ•¸é‡: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… æå¤±å‡½æ•¸: BCEWithLogitsLoss\n",
                        "âœ… å„ªåŒ–å™¨: Adam (lr=0.0001)\n"
                    ]
                }
            ],
            "source": [
                "# è¨­ç½®è¨“ç·´çµ„ä»¶\n",
                "criterion = nn.BCEWithLogitsLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
                "\n",
                "print(f\"âœ… æå¤±å‡½æ•¸: BCEWithLogitsLoss\")\n",
                "print(f\"âœ… å„ªåŒ–å™¨: Adam (lr={LR})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… è¨“ç·´å‡½æ•¸å®šç¾©å®Œæˆ\n"
                    ]
                }
            ],
            "source": [
                "# è¨“ç·´å‡½æ•¸ï¼ˆåŒ…å« GPU è¨˜æ†¶é«”ç®¡ç†ï¼‰\n",
                "def train_epoch(model, loader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch_idx, (images, masks) in enumerate(pbar):\n",
                "        try:\n",
                "            images = images.to(device)\n",
                "            masks = masks.to(device)\n",
                "            \n",
                "            # Forward pass\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, masks)\n",
                "            \n",
                "            # Backward pass\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "            \n",
                "            # å®šæœŸæ¸…ç† GPU è¨˜æ†¶é«”ï¼ˆé‡è¦ï¼ï¼‰\n",
                "            if device == 'cuda' and batch_idx % 50 == 0:\n",
                "                torch.cuda.empty_cache()\n",
                "            \n",
                "        except RuntimeError as e:\n",
                "            if \"out of memory\" in str(e) or \"timeout\" in str(e):\n",
                "                print(f\"\\nâŒ GPU éŒ¯èª¤: {e}\")\n",
                "                print(\"   æ­£åœ¨æ¸…ç†è¨˜æ†¶é«”...\")\n",
                "                if device == 'cuda':\n",
                "                    torch.cuda.empty_cache()\n",
                "                    gc.collect()\n",
                "                print(\"   å»ºè­°: é™ä½ batch size æˆ–åˆ‡æ›åˆ° CPU\")\n",
                "                raise\n",
                "            else:\n",
                "                raise\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "print(\"âœ… è¨“ç·´å‡½æ•¸å®šç¾©å®Œæˆ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "é–‹å§‹è¨“ç·´...\n",
                        "è¨­å‚™: cuda\n",
                        "Epochs: 25\n",
                        "Batch Size: 2\n",
                        "Image Size: 256x256\n",
                        "============================================================\n",
                        "\n",
                        "Epoch 1/25\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5670c65490884c038b2ac106a17cf76d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Training:   0%|          | 0/751 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# è¨“ç·´\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# ä¿å­˜æœ€ä½³æ¨¡å‹\u001b[39;00m\n",
                        "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# å®šæœŸæ¸…ç† GPU è¨˜æ†¶é«”ï¼ˆé‡è¦ï¼ï¼‰\u001b[39;00m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# é–‹å§‹è¨“ç·´\n",
                "print(\"=\"*60)\n",
                "print(\"é–‹å§‹è¨“ç·´...\")\n",
                "print(f\"è¨­å‚™: {DEVICE}\")\n",
                "print(f\"Epochs: {EPOCHS}\")\n",
                "print(f\"Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "best_loss = float('inf')\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    try:\n",
                "        # è¨“ç·´\n",
                "        train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
                "        print(f\"Average Loss: {train_loss:.4f}\")\n",
                "        \n",
                "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
                "        if train_loss < best_loss:\n",
                "            best_loss = train_loss\n",
                "            torch.save(model.state_dict(), 'best_model.pth')\n",
                "            print(f\"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {best_loss:.4f})\")\n",
                "        \n",
                "        # å®šæœŸæ¸…ç†è¨˜æ†¶é«”\n",
                "        if DEVICE == 'cuda':\n",
                "            torch.cuda.empty_cache()\n",
                "            gc.collect()\n",
                "            \n",
                "    except RuntimeError as e:\n",
                "        print(f\"\\nâŒ è¨“ç·´å¤±æ•—: {e}\")\n",
                "        print(\"\\nå»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ:\")\n",
                "        print(\"1. é‡å•Ÿ Jupyter kernel\")\n",
                "        print(\"2. å°‡ BATCH_SIZE é™åˆ° 1\")\n",
                "        print(\"3. å°‡ IMG_SIZE é™åˆ° 128\")\n",
                "        print(\"4. ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆä¿®æ”¹ DEVICE = 'cpu'ï¼‰\")\n",
                "        break\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"è¨“ç·´å®Œæˆï¼\")\n",
                "print(f\"æœ€ä½³æå¤±: {best_loss:.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ‰ å®Œæˆï¼\n",
                "\n",
                "å¦‚æœè¨“ç·´æˆåŠŸå®Œæˆï¼š\n",
                "- âœ… æ‚¨çš„æ¨¡å‹å·²ä¿å­˜ç‚º `best_model.pth`\n",
                "- âœ… æ‚¨å¯ä»¥ä½¿ç”¨é€™å€‹æ¨¡å‹é€²è¡Œé æ¸¬\n",
                "\n",
                "å¦‚æœé‡åˆ°éŒ¯èª¤ï¼š\n",
                "1. é‡å•Ÿ Jupyter kernel\n",
                "2. å°‡ `BATCH_SIZE` è¨­ç‚º 1\n",
                "3. æˆ–è€…åˆ‡æ›åˆ° CPU æ¨¡å¼"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "brain_tumor",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
